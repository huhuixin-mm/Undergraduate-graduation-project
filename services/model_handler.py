"""
Optimized model handler for Meditron3-Qwen2.5
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from typing import Optional, List, Dict, Any
from pathlib import Path
import sys
from loguru import logger

# Add project root to path
sys.path.append(str(Path(__file__).parent.parent))
from config.settings import settings, get_project_root


class MeditronModel:
    """Meditron3-Qwen2.5æ¨¡å‹å¤„ç†å™¨"""
    
    def __init__(self, model_path: Optional[str] = None):
        self.model_path = model_path or str(get_project_root() / settings.model.model_path)
        self.model = None
        self.tokenizer = None
        self.device = self._setup_device()
        self.is_loaded = False
        
        # è®¾ç½®æ—¥å¿—
        logger.add(
            get_project_root() / "logs" / "model.log",
            level=settings.logging.log_level,
            rotation="10 MB"
        )
    
    def _setup_device(self) -> str:
        """è®¾ç½®å¹¶è¿”å›æœ€ä¼˜GPUè®¾å¤‡"""
        if not torch.cuda.is_available():
            logger.warning("âš ï¸ CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPUï¼ˆæ¨ç†è¾ƒæ…¢ï¼‰")
            return "cpu"
        return "cuda:0"
    
    def _get_quantization_config(self) -> Optional[BitsAndBytesConfig]:
        """è·å–é‡åŒ–é…ç½®ä»¥ä¼˜åŒ–æ˜¾å­˜ä½¿ç”¨"""
        if settings.model.load_in_4bit:
            return BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4"
            )
        elif settings.model.load_in_8bit:
            return BitsAndBytesConfig(load_in_8bit=True)
        return None
    
    def load_model(self) -> bool:
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
        try:
            logger.info("ğŸš€ åŠ è½½ Meditron3-Qwen2.5 æ¨¡å‹...")
            
            # æ£€æŸ¥æ¨¡å‹è·¯å¾„
            if not Path(self.model_path).exists():
                logger.error(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.model_path}")
                return False
            
            # åŠ è½½åˆ†è¯å™¨
            logger.info("ğŸ“š åŠ è½½åˆ†è¯å™¨...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                trust_remote_code=True,
                use_fast=True
            )
            
            # è®¾ç½®å¡«å……token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            logger.info(f"âœ… åˆ†è¯å™¨åŠ è½½å®Œæˆ: vocab_size={self.tokenizer.vocab_size}")
            
            # å‡†å¤‡æ¨¡å‹åŠ è½½å‚æ•°
            model_kwargs = {
                "dtype": getattr(torch, settings.model.dtype),
                "device_map": self.device if self.device.startswith("cuda:") else "auto",
                "trust_remote_code": True,
                "low_cpu_mem_usage": True,
            }
            
            # æ·»åŠ é‡åŒ–é…ç½®
            quantization_config = self._get_quantization_config()
            if quantization_config:
                model_kwargs["quantization_config"] = quantization_config
                logger.info("ğŸ”§ ä½¿ç”¨é‡åŒ–ä¼˜åŒ–æ˜¾å­˜")
            
            # åŠ è½½æ¨¡å‹
            logger.info("ğŸ§  åŠ è½½æ¨¡å‹ï¼ˆè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰...")
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                **model_kwargs
            )
            
            self.model.eval()
            self.is_loaded = True
            
            # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
            total_params = sum(p.numel() for p in self.model.parameters())
            logger.info(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            logger.info(f"ğŸ“Š æ€»å‚æ•°é‡: {total_params:,}")
            
            # æ¸…ç†ç¼“å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def generate_response(
        self,
        prompt: str,
        max_new_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        repetition_penalty: Optional[float] = None,
        stream: bool = False,
        **kwargs
    ):
        """
        ç”Ÿæˆå›å¤ - æ”¯æŒæ™®é€šå’Œæµå¼è¾“å‡º
        
        Args:
            prompt: è¾“å…¥æç¤º
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            top_p: top-pé‡‡æ ·å‚æ•°
            stream: æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡º (é»˜è®¤False)
            **kwargs: å…¶ä»–ç”Ÿæˆå‚æ•°
        
        Returns:
            str: æ™®é€šæ¨¡å¼è¿”å›å®Œæ•´å›å¤å­—ç¬¦ä¸²
            Generator: æµå¼æ¨¡å¼è¿”å›ç”Ÿæˆå™¨ï¼Œé€æ­¥è¾“å‡ºæ–‡æœ¬
        """
        if not self.is_loaded:
            raise RuntimeError("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨ load_model()")
        
        # è®¾ç½®é»˜è®¤å‚æ•°
        max_new_tokens = max_new_tokens or 512
        temperature = temperature or settings.model.temperature
        top_p = top_p or settings.model.top_p
        repetition_penalty = repetition_penalty or settings.model.repetition_penalty
        
        # å‡†å¤‡ç”Ÿæˆå‚æ•°
        generation_params = {
            "max_new_tokens": max_new_tokens,
            "temperature": temperature,
            "top_p": top_p,
            "repetition_penalty": repetition_penalty,
            **kwargs  # å…è®¸ä¼ å…¥å…¶ä»–å‚æ•°
        }
        
        try:
            # ç¼–ç è¾“å…¥
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                add_special_tokens=True,
                padding=True,
                truncation=True,
                max_length=settings.model.max_length
            )
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            if self.device.startswith("cuda"):
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # ç”Ÿæˆå›å¤
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    **generation_params,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            # è§£ç å›å¤
            response = self.tokenizer.decode(
                outputs[0][len(inputs["input_ids"][0]):],
                skip_special_tokens=True
            ).strip()
            
            # è°ƒè¯•ä¿¡æ¯
            if len(response) == 0:
                logger.warning(f"âš ï¸ ç”Ÿæˆäº†ç©ºå›å¤ï¼Œè¾“å…¥é•¿åº¦: {len(inputs['input_ids'][0])}, è¾“å‡ºé•¿åº¦: {len(outputs[0])}")
                logger.warning(f"âš ï¸ ç”Ÿæˆå‚æ•°: {generation_params}")
                # å°è¯•ä¸è·³è¿‡ç‰¹æ®Štokençš„è§£ç 
                raw_response = self.tokenizer.decode(outputs[0][len(inputs["input_ids"][0]):])
                logger.warning(f"âš ï¸ åŸå§‹è¾“å‡º: '{raw_response}'")
            
            # æ ¹æ®streamå‚æ•°å†³å®šè¿”å›æ–¹å¼
            if stream:
                return self._create_stream_generator(response)
            else:
                return response
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆå›å¤æ—¶å‡ºé”™: {e}")
            raise
    
    def _create_stream_generator(self, full_response: str):
        """åˆ›å»ºæµå¼ç”Ÿæˆå™¨"""
        # æŒ‰è¯åˆ†å‰²è¿›è¡Œæµå¼è¾“å‡º
        words = full_response.split()
        current_text = ""
        
        for word in words:
            current_text += word + " "
            yield current_text.strip()
        
        # ç¡®ä¿æœ€åè¿”å›å®Œæ•´å“åº”
        if current_text.strip() != full_response:
            yield full_response
    
    def chat(self, messages: List[Dict[str, str]], stream: bool = False, **kwargs):
        """
        å¤šè½®å¯¹è¯æ¥å£
        
        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            stream: æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡º (é»˜è®¤False)
            **kwargs: å…¶ä»–ç”Ÿæˆå‚æ•°
        
        Returns:
            str: æ™®é€šæ¨¡å¼è¿”å›å®Œæ•´å›å¤å­—ç¬¦ä¸²
            Generator: æµå¼æ¨¡å¼è¿”å›ç”Ÿæˆå™¨ï¼Œé€æ­¥è¾“å‡ºæ–‡æœ¬
        """
        if not self.is_loaded:
            raise RuntimeError("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨ load_model()")
        
        # æ„å»ºå¯¹è¯æç¤º
        prompt_parts = []
        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            
            if role == "user":
                prompt_parts.append(f"ç”¨æˆ·: {content}")
            elif role == "assistant":
                prompt_parts.append(f"åŠ©æ‰‹: {content}")
            elif role == "system":
                prompt_parts.append(f"ç³»ç»Ÿ: {content}")
        
        prompt_parts.append("åŠ©æ‰‹: ")
        prompt = "\n".join(prompt_parts)
        
        return self.generate_response(prompt, stream=stream, **kwargs)
    
    def unload_model(self):
        """å¸è½½æ¨¡å‹é‡Šæ”¾æ˜¾å­˜"""
        if self.model is not None:
            del self.model
            self.model = None
        
        if self.tokenizer is not None:
            del self.tokenizer
            self.tokenizer = None
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        self.is_loaded = False
        logger.info("ğŸ—‘ï¸ æ¨¡å‹å·²å¸è½½ï¼Œæ˜¾å­˜å·²é‡Šæ”¾")
    
    def __del__(self):
        """ææ„å‡½æ•°ï¼šè‡ªåŠ¨æ¸…ç†èµ„æº"""
        self.unload_model()


# å…¨å±€æ¨¡å‹å®ä¾‹
_global_model: Optional[MeditronModel] = None


def get_model() -> MeditronModel:
    """è·å–å…¨å±€æ¨¡å‹å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _global_model
    if _global_model is None:
        _global_model = MeditronModel()
    return _global_model


def ensure_model_loaded() -> bool:
    """ç¡®ä¿æ¨¡å‹å·²åŠ è½½"""
    model = get_model()
    if not model.is_loaded:
        return model.load_model()
    return True


# ä¸»å‡½æ•°ç”¨äºæµ‹è¯•
if __name__ == "__main__":
    logger.info("ğŸ§ª å¼€å§‹æ¨¡å‹æµ‹è¯•...")
    
    model = MeditronModel()
    if model.load_model():
        logger.info("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œå¼€å§‹æµ‹è¯•æ¨ç†...")
        
        test_prompt = "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹ECS (Extracellular Space) çš„ä½œç”¨ã€‚"
        response = model.generate_response(test_prompt, max_new_tokens=100)
        
        logger.info(f"ğŸ“ æµ‹è¯•æç¤º: {test_prompt}")
        logger.info(f"ğŸ¤– æ¨¡å‹å›å¤: {response}")
        
        model.unload_model()
        logger.info("ğŸ‰ æµ‹è¯•å®Œæˆï¼")
    else:
        logger.error("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼")