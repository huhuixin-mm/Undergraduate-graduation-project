#!/usr/bin/env python3
"""
å…¨é¢æµ‹è¯• Meditron3-Qwen2.5 æ¨¡å‹åŠŸèƒ½
æµ‹è¯•å†…å®¹ï¼šå•è½®å¯¹è¯ã€å¤šè½®å¯¹è¯ã€æµå¼ç”Ÿæˆã€å¤šè¯­è¨€æ”¯æŒ
"""

import sys
import time
from pathlib import Path
from typing import List, Dict

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent))

from services.model_handler import MeditronModel, get_model, ensure_model_loaded


class ModelTester:
    """æ¨¡å‹æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.model = None
        
    def setup(self):
        """åˆå§‹åŒ–è®¾ç½®"""
        print("ğŸš€ åˆå§‹åŒ– Meditron3-Qwen2.5 æµ‹è¯•...")
        print(f"ğŸ“ æµ‹è¯•æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        self.model = MeditronModel()
        success = self.model.load_model()
        
        if success:
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
            print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.model.device}")
            return True
        else:
            print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥!")
            return False
    
    def test_single_conversation(self):
        """æµ‹è¯•1: å•è½®å¯¹è¯"""
        print("\n" + "="*60)
        print("ğŸ§ª æµ‹è¯• 1: å•è½®å¯¹è¯")
        print("="*60)
        
        test_prompts = [
            {
                "prompt": "è¯·è¯¦ç»†è§£é‡Šä»€ä¹ˆæ˜¯é«˜è¡€å‹ï¼ŒåŒ…æ‹¬ç—‡çŠ¶å’Œé¢„é˜²æªæ–½ã€‚",
                "desc": "ä¸­æ–‡åŒ»ç–—é—®é¢˜"
            },
            {
                "prompt": "æˆ‘æœ€è¿‘æ€»æ˜¯å¤±çœ ï¼Œæœ‰ä»€ä¹ˆå¥½çš„å»ºè®®å—ï¼Ÿ",
                "desc": "ä¸­æ–‡å¥åº·å’¨è¯¢"
            },
            {
                "prompt": "é˜¿å¸åŒ¹æ—çš„ä¸»è¦ä½œç”¨å’Œå‰¯ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ",
                "desc": "ä¸­æ–‡è¯ç‰©å’¨è¯¢"
            }
        ]
        
        for i, test_case in enumerate(test_prompts, 1):
            print(f"\n--- å•è½®å¯¹è¯æµ‹è¯• {i}: {test_case['desc']} ---")
            print(f"ğŸ‘¤ ç”¨æˆ·: {test_case['prompt']}")
            
            try:
                start_time = time.time()
                response = self.model.generate_response(
                    test_case['prompt'],
                    max_new_tokens=200,
                    temperature=0.7,
                    top_p=0.9
                )
                end_time = time.time()
                
                print(f"ğŸ¤– åŠ©æ‰‹: {response}")
                print(f"â±ï¸ å“åº”æ—¶é—´: {end_time - start_time:.2f}ç§’")
                print("âœ… æµ‹è¯•é€šè¿‡")
                
            except Exception as e:
                print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
    
    def test_multi_turn_conversation(self):
        """æµ‹è¯•2: å¤šè½®å¯¹è¯"""
        print("\n" + "="*60)
        print("ğŸ§ª æµ‹è¯• 2: å¤šè½®å¯¹è¯")
        print("="*60)
        
        # ç¬¬ä¸€ä¸ªå¤šè½®å¯¹è¯åœºæ™¯
        print("\n--- å¤šè½®å¯¹è¯åœºæ™¯ 1: å¤´ç—›é—®é¢˜å’¨è¯¢ ---")
        
        conversation_1 = [
            {"role": "user", "content": "åŒ»ç”Ÿæ‚¨å¥½ï¼Œæˆ‘æœ€è¿‘ç»å¸¸å¤´ç—›ï¼Œç‰¹åˆ«æ˜¯ä¸‹åˆçš„æ—¶å€™ã€‚"},
            {"role": "assistant", "content": ""},  # å°†è¢«å¡«å……
            {"role": "user", "content": "æˆ‘å¹³æ—¶å·¥ä½œéœ€è¦é•¿æ—¶é—´çœ‹ç”µè„‘ï¼Œè¿™ä¼šæ˜¯åŸå› å—ï¼Ÿ"},
            {"role": "assistant", "content": ""},  # å°†è¢«å¡«å……
            {"role": "user", "content": "é‚£æˆ‘åº”è¯¥æ€ä¹ˆé¢„é˜²å’Œç¼“è§£å‘¢ï¼Ÿ"}
        ]
        
        self._run_multi_turn_conversation(conversation_1, "å¤´ç—›å’¨è¯¢")
        
        # ç¬¬äºŒä¸ªå¤šè½®å¯¹è¯åœºæ™¯
        print("\n--- å¤šè½®å¯¹è¯åœºæ™¯ 2: ç³–å°¿ç—…çŸ¥è¯†å’¨è¯¢ ---")
        
        conversation_2 = [
            {"role": "user", "content": "æˆ‘æƒ³äº†è§£ä¸€ä¸‹ç³–å°¿ç—…çš„åŸºæœ¬çŸ¥è¯†ã€‚"},
            {"role": "assistant", "content": ""},  # å°†è¢«å¡«å……
            {"role": "user", "content": "é‚£ç³–å°¿ç—…æ‚£è€…åœ¨é¥®é£Ÿä¸Šéœ€è¦æ³¨æ„ä»€ä¹ˆï¼Ÿ"},
            {"role": "assistant", "content": ""},  # å°†è¢«å¡«å……
            {"role": "user", "content": "å¦‚æœè¡€ç³–æ§åˆ¶ä¸å¥½ä¼šæœ‰ä»€ä¹ˆåæœï¼Ÿ"}
        ]
        
        self._run_multi_turn_conversation(conversation_2, "ç³–å°¿ç—…å’¨è¯¢")
    
    def _run_multi_turn_conversation(self, conversation: List[Dict], scenario_name: str):
        """è¿è¡Œå¤šè½®å¯¹è¯"""
        messages = []
        
        for turn_idx, turn in enumerate(conversation):
            if turn["role"] == "user":
                messages.append(turn)
                print(f"\nğŸ—£ï¸ ç¬¬ {len([m for m in messages if m['role'] == 'user'])} è½®")
                print(f"ğŸ‘¤ ç”¨æˆ·: {turn['content']}")
                
                try:
                    start_time = time.time()
                    response = self.model.chat(
                        messages,
                        max_new_tokens=180,
                        temperature=0.7,
                        top_p=0.9
                    )
                    end_time = time.time()
                    
                    messages.append({"role": "assistant", "content": response})
                    print(f"ğŸ¤– åŠ©æ‰‹: {response}")
                    print(f"â±ï¸ å“åº”æ—¶é—´: {end_time - start_time:.2f}ç§’")
                    
                except Exception as e:
                    print(f"âŒ ç¬¬{len([m for m in messages if m['role'] == 'user'])}è½®å¯¹è¯å¤±è´¥: {e}")
                    break
        
        print(f"âœ… {scenario_name}å¤šè½®å¯¹è¯æµ‹è¯•å®Œæˆ")
    
    def test_streaming_generation(self):
        """æµ‹è¯•3: æµå¼ç”Ÿæˆ"""
        print("\n" + "="*60)
        print("ğŸ§ª æµ‹è¯• 3: æµå¼ç”Ÿæˆ")
        print("="*60)
        
        test_prompts = [
            "è¯·è¯¦ç»†ä»‹ç»ä¸€ä¸‹å¿ƒè„ç—…çš„é¢„é˜²æªæ–½ï¼ŒåŒ…æ‹¬é¥®é£Ÿã€è¿åŠ¨å’Œç”Ÿæ´»æ–¹å¼çš„å»ºè®®ã€‚",
            "ä»€ä¹ˆæ˜¯æŠ‘éƒç—‡ï¼Ÿå®ƒçš„ç—‡çŠ¶ã€åŸå› å’Œæ²»ç–—æ–¹æ³•æœ‰å“ªäº›ï¼Ÿ"
        ]
        
        for i, prompt in enumerate(test_prompts, 1):
            print(f"\n--- æµå¼ç”Ÿæˆæµ‹è¯• {i} ---")
            print(f"ğŸ‘¤ ç”¨æˆ·: {prompt}")
            print("ğŸ¤– åŠ©æ‰‹: ", end="", flush=True)
            
            try:
                start_time = time.time()
                full_response = ""
                
                for chunk in self.model.generate_response(
                    prompt,
                    max_new_tokens=250,
                    temperature=0.7,
                    top_p=0.9,
                    stream=True  # å¯ç”¨æµå¼æ¨¡å¼
                ):
                    # åªæ˜¾ç¤ºæ–°å¢çš„éƒ¨åˆ†
                    new_part = chunk[len(full_response):]
                    print(new_part, end="", flush=True)
                    full_response = chunk
                    time.sleep(0.02)  # æ¨¡æ‹Ÿæµå¼å»¶è¿Ÿ
                
                end_time = time.time()
                print(f"\nâ±ï¸ æ€»å“åº”æ—¶é—´: {end_time - start_time:.2f}ç§’")
                print(f"ğŸ“Š æœ€ç»ˆå›å¤é•¿åº¦: {len(full_response)}å­—ç¬¦")
                print("âœ… æµå¼ç”Ÿæˆæµ‹è¯•é€šè¿‡")
                
            except Exception as e:
                print(f"\nâŒ æµå¼ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
    
    def test_multilingual_conversation(self):
        """æµ‹è¯•4: å¤šè¯­è¨€å¯¹è¯"""
        print("\n" + "="*60)
        print("ğŸ§ª æµ‹è¯• 4: å¤šè¯­è¨€å¯¹è¯")
        print("="*60)
        
        multilingual_tests = [
            {
                "language": "ä¸­æ–‡",
                "prompt": "è¯·è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯æ–°å† ç—…æ¯’ï¼Œä»¥åŠå¦‚ä½•é¢„é˜²æ„ŸæŸ“ï¼Ÿ",
                "expected_lang": "zh"
            },
            {
                "language": "English",
                "prompt": "What are the main symptoms of diabetes and how can it be managed?",
                "expected_lang": "en"
            },
            {
                "language": "ä¸­è‹±æ··åˆ",
                "prompt": "Doctorï¼Œæˆ‘æƒ³é—®ä¸€ä¸‹å…³äºCOVID-19ç–«è‹—æ¥ç§çš„é—®é¢˜ï¼Œæœ‰ä»€ä¹ˆå‰¯ä½œç”¨å—ï¼Ÿ",
                "expected_lang": "mixed"
            },
            {
                "language": "è‹±æ–‡åŒ»å­¦æœ¯è¯­",
                "prompt": "Can you explain the difference between Type 1 and Type 2 diabetes mellitus?",
                "expected_lang": "en"
            },
            {
                "language": "ä¸­æ–‡ä¸“ä¸šæœ¯è¯­",
                "prompt": "è¯·è¯¦ç»†è¯´æ˜é«˜è¡€å‹çš„ç—…ç†ç”Ÿç†æœºåˆ¶å’Œè¯ç‰©æ²»ç–—åŸç†ã€‚",
                "expected_lang": "zh"
            }
        ]
        
        for i, test_case in enumerate(multilingual_tests, 1):
            print(f"\n--- å¤šè¯­è¨€æµ‹è¯• {i}: {test_case['language']} ---")
            print(f"ğŸ‘¤ ç”¨æˆ· ({test_case['language']}): {test_case['prompt']}")
            
            try:
                start_time = time.time()
                
                response = self.model.generate_response(
                    test_case['prompt'],
                    max_new_tokens=200,
                    temperature=0.7,
                    top_p=0.9
                )
                end_time = time.time()
                
                print(f"ğŸ¤– åŠ©æ‰‹: {response}")
                print(f"â±ï¸ å“åº”æ—¶é—´: {end_time - start_time:.2f}ç§’")
                
            except Exception as e:
                print(f"âŒ å¤šè¯­è¨€æµ‹è¯• {i} å¤±è´¥: {e}")
    
    def test_conversation_context_memory(self):
        """æµ‹è¯•5: å¯¹è¯ä¸Šä¸‹æ–‡è®°å¿†"""
        print("\n" + "="*60)
        print("ğŸ§ª æµ‹è¯• 5: å¯¹è¯ä¸Šä¸‹æ–‡è®°å¿†")
        print("="*60)
        
        print("\n--- ä¸Šä¸‹æ–‡è®°å¿†æµ‹è¯• ---")
        
        # æ„å»ºä¸€ä¸ªéœ€è¦è®°ä½ä¸Šä¸‹æ–‡çš„å¯¹è¯
        context_test = [
            {"role": "user", "content": "æˆ‘æ˜¯ä¸€ä¸ª45å²çš„ç”·æ€§ï¼Œæœ€è¿‘ä½“æ£€å‘ç°è¡€å‹æœ‰ç‚¹é«˜ã€‚"},
            {"role": "user", "content": "åŸºäºæˆ‘åˆšæ‰æåˆ°çš„æƒ…å†µï¼Œæˆ‘åº”è¯¥æ³¨æ„ä»€ä¹ˆï¼Ÿ"},
            {"role": "user", "content": "æ ¹æ®æˆ‘çš„å¹´é¾„å’Œæ€§åˆ«ï¼Œè¿˜æœ‰å…¶ä»–éœ€è¦å®šæœŸæ£€æŸ¥çš„é¡¹ç›®å—ï¼Ÿ"}
        ]
        
        messages = []
        
        for turn_idx, user_msg in enumerate(context_test, 1):
            messages.append(user_msg)
            print(f"\nğŸ—£ï¸ ç¬¬ {turn_idx} è½®")
            print(f"ğŸ‘¤ ç”¨æˆ·: {user_msg['content']}")
            
            try:
                start_time = time.time()
                response = self.model.chat(
                    messages,
                    max_new_tokens=180,
                    temperature=0.7,
                    top_p=0.9
                )
                end_time = time.time()
                
                messages.append({"role": "assistant", "content": response})
                print(f"ğŸ¤– åŠ©æ‰‹: {response}")
                print(f"â±ï¸ å“åº”æ—¶é—´: {end_time - start_time:.2f}ç§’")
                
                # æ£€æŸ¥æ˜¯å¦è®°ä½äº†ä¸Šä¸‹æ–‡
                if turn_idx > 1:
                    context_keywords = ["45", "ç”·æ€§", "è¡€å‹", "é«˜"]
                    context_maintained = any(keyword in response for keyword in context_keywords)
                    print(f"ğŸ§  ä¸Šä¸‹æ–‡è®°å¿†: {'âœ“' if context_maintained else '?'}")
                
            except Exception as e:
                print(f"âŒ ç¬¬ {turn_idx} è½®ä¸Šä¸‹æ–‡æµ‹è¯•å¤±è´¥: {e}")
                break
        
        print("âœ… ä¸Šä¸‹æ–‡è®°å¿†æµ‹è¯•å®Œæˆ")
    
    def test_streaming_chat(self):
        """æµ‹è¯•6: æµå¼å¤šè½®å¯¹è¯"""
        print("\n" + "="*60)
        print("ğŸ§ª æµ‹è¯• 6: æµå¼å¤šè½®å¯¹è¯")
        print("="*60)
        
        print("\n--- æµå¼å¯¹è¯æµ‹è¯• ---")
        
        # æ„å»ºæµå¼å¯¹è¯æµ‹è¯•
        streaming_messages = [
            {"role": "user", "content": "åŒ»ç”Ÿï¼Œæˆ‘æƒ³äº†è§£ä¸€ä¸‹å…³äºå‡è‚¥çš„å¥åº·æ–¹æ³•ã€‚"},
            {"role": "user", "content": "é‚£å¯¹äºæˆ‘è¿™ç§å·¥ä½œæ¯”è¾ƒå¿™çš„äººï¼Œæœ‰ä»€ä¹ˆç®€å•æ˜“è¡Œçš„å»ºè®®å—ï¼Ÿ"}
        ]
        
        messages = []
        
        for turn_idx, user_msg in enumerate(streaming_messages, 1):
            messages.append(user_msg)
            print(f"\nğŸ—£ï¸ ç¬¬ {turn_idx} è½® - æµå¼å¯¹è¯")
            print(f"ğŸ‘¤ ç”¨æˆ·: {user_msg['content']}")
            print("ğŸ¤– åŠ©æ‰‹: ", end="", flush=True)
            
            try:
                start_time = time.time()
                full_response = ""
                
                # ä½¿ç”¨æµå¼å¯¹è¯
                for chunk in self.model.chat(
                    messages,
                    max_new_tokens=180,
                    temperature=0.7,
                    top_p=0.9,
                    stream=True  # å¯ç”¨æµå¼æ¨¡å¼
                ):
                    # åªæ˜¾ç¤ºæ–°å¢çš„éƒ¨åˆ†
                    new_part = chunk[len(full_response):]
                    print(new_part, end="", flush=True)
                    full_response = chunk
                    time.sleep(0.03)  # æ¨¡æ‹Ÿæµå¼å»¶è¿Ÿ
                
                end_time = time.time()
                
                messages.append({"role": "assistant", "content": full_response})
                print(f"\nâ±ï¸ å“åº”æ—¶é—´: {end_time - start_time:.2f}ç§’")
                print(f"ğŸ“Š å›å¤é•¿åº¦: {len(full_response)}å­—ç¬¦")
                
            except Exception as e:
                print(f"\nâŒ ç¬¬ {turn_idx} è½®æµå¼å¯¹è¯å¤±è´¥: {e}")
                break
        
        print("âœ… æµå¼å¯¹è¯æµ‹è¯•å®Œæˆ")
    
    def run_performance_benchmark(self):
        """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print("\n" + "="*60)
        print("ğŸƒ æ€§èƒ½åŸºå‡†æµ‹è¯•")
        print("="*60)
        
        benchmark_prompts = [
            "è¯·ç®€å•ä»‹ç»æ„Ÿå†’çš„ç—‡çŠ¶ã€‚",
            "ä»€ä¹ˆæ˜¯é«˜è¡€å‹ï¼Ÿ",
            "ç³–å°¿ç—…æ‚£è€…é¥®é£Ÿæ³¨æ„äº‹é¡¹ã€‚",
            "å¦‚ä½•é¢„é˜²å¿ƒè„ç—…ï¼Ÿ",
            "æŠ‘éƒç—‡çš„æ²»ç–—æ–¹æ³•æœ‰å“ªäº›ï¼Ÿ"
        ]
        
        total_time = 0
        total_tokens = 0
        
        print(f"\nğŸ“Š è¿è¡Œ {len(benchmark_prompts)} ä¸ªåŸºå‡†æµ‹è¯•...")
        
        for i, prompt in enumerate(benchmark_prompts, 1):
            print(f"\nâš¡ åŸºå‡†æµ‹è¯• {i}/{len(benchmark_prompts)}")
            
            try:
                start_time = time.time()
                response = self.model.generate_response(
                    prompt,
                    max_new_tokens=100,
                    temperature=0.7
                )
                end_time = time.time()
                
                response_time = end_time - start_time
                token_count = len(self.model.tokenizer.encode(response))
                
                total_time += response_time
                total_tokens += token_count
                
                print(f"  â±ï¸ å“åº”æ—¶é—´: {response_time:.2f}ç§’")
                print(f"  ğŸ“ ç”Ÿæˆtokenæ•°: {token_count}")
                print(f"  ğŸš€ ååé‡: {token_count/response_time:.2f} tokens/ç§’")
                
            except Exception as e:
                print(f"  âŒ åŸºå‡†æµ‹è¯• {i} å¤±è´¥: {e}")
        
        # è®¡ç®—å¹³å‡æ€§èƒ½
        avg_time = total_time / len(benchmark_prompts)
        avg_throughput = total_tokens / total_time
        
        print(f"\nğŸ“ˆ æ€§èƒ½æ€»ç»“:")
        print(f"  å¹³å‡å“åº”æ—¶é—´: {avg_time:.2f}ç§’")
        print(f"  æ€»ç”Ÿæˆtokenæ•°: {total_tokens}")
        print(f"  å¹³å‡ååé‡: {avg_throughput:.2f} tokens/ç§’")
        print("âœ… æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆ")
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.model:
            self.model.unload_model()
            print("\nğŸ—‘ï¸ æ¨¡å‹å·²å¸è½½ï¼Œèµ„æºå·²æ¸…ç†")
    
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸ¯ å¼€å§‹å…¨é¢æµ‹è¯• Meditron3-Qwen2.5 æ¨¡å‹")
        print("="*80)
        
        if not self.setup():
            return False
        
        try:
            # è¿è¡Œæ‰€æœ‰æµ‹è¯•
            self.test_single_conversation()
            self.test_multi_turn_conversation()
            self.test_streaming_generation()
            self.test_multilingual_conversation()
            self.test_conversation_context_memory()
            self.test_streaming_chat()  # æ–°å¢æµå¼å¯¹è¯æµ‹è¯•
            self.run_performance_benchmark()
            
            print("\n" + "="*80)
            print("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
            print("âœ… Meditron3-Qwen2.5 æ¨¡å‹åŠŸèƒ½éªŒè¯é€šè¿‡")
            print("="*80)
            
            return True
            
        except KeyboardInterrupt:
            print("\nâš ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
            return False
        except Exception as e:
            print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return False
        finally:
            self.cleanup()


def main():
    """ä¸»å‡½æ•°"""
    tester = ModelTester()
    success = tester.run_all_tests()
    
    exit_code = 0 if success else 1
    sys.exit(exit_code)


if __name__ == "__main__":
    main()